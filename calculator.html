<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SelfHostLLM - GPU Memory Calculator for LLM Inference</title>

    <!-- Primary Meta Tags -->
    <meta name="title" content="SelfHostLLM - GPU Memory Calculator for LLM Inference">
    <meta name="description"
        content="Calculate GPU memory requirements and max concurrent requests for self-hosted LLM inference. Support for Llama, Qwen, DeepSeek, Mistral and more. Plan your AI infrastructure efficiently.">
    <meta name="keywords"
        content="LLM, GPU calculator, VRAM calculator, self-hosted AI, Llama, Qwen, DeepSeek, Mistral, GPU memory, inference calculator, AI infrastructure">
    <meta name="author" content="Eran Sandler">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://selfhostllm.org/">
    <meta property="og:title" content="SelfHostLLM - GPU Memory Calculator for LLM Inference">
    <meta property="og:description"
        content="Calculate GPU memory requirements and max concurrent requests for self-hosted LLM inference. Support for Llama, Qwen, DeepSeek, Mistral and more.">
    <meta property="og:image" content="https://selfhostllm.org/og-image.png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:site_name" content="SelfHostLLM">

    <!-- Twitter / X -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:url" content="https://selfhostllm.org/">
    <meta name="twitter:title" content="SelfHostLLM - GPU Memory Calculator for LLM Inference">
    <meta name="twitter:description"
        content="Calculate GPU memory requirements and max concurrent requests for self-hosted LLM inference. Plan your AI infrastructure efficiently.">
    <meta name="twitter:image" content="https://selfhostllm.org/og-image.png">
    <meta name="twitter:creator" content="@erans">

    <!-- Additional Meta Tags -->
    <meta name="robots" content="index, follow">
    <meta name="language" content="English">
    <meta name="theme-color" content="#00ff00">
    <link rel="canonical" href="https://selfhostllm.org/">

    <!-- Favicon -->
    <link rel="icon" type="image/x-icon" href="/favicon.ico">

    <!-- Fonts -->
    <link
        href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap"
        rel="stylesheet">

    <!-- Tailwind CSS (for shared header/footer) -->
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    colors: {
                        'navy': '#1a2332',
                        'electric': '#00d4ff',
                        'amber': '#ffb347',
                        'sage': '#7fb069',
                        'charcoal': '#2d3748',
                        'soft-gray': '#e2e8f0',
                        'deep-charcoal': '#1a202c'
                    },
                    fontFamily: {
                        'mono': ['JetBrains Mono', 'monospace'],
                        'sans': ['Inter', 'sans-serif']
                    },
                    animation: {
                        'pulse-slow': 'pulse 3s cubic-bezier(0.4, 0, 0.6, 1) infinite',
                        'float': 'float 6s ease-in-out infinite',
                        'glow': 'glow 2s ease-in-out infinite alternate'
                    }
                }
            }
        }
    </script>

    <link rel="stylesheet" href="selfhost-llm.css">
</head>

<body class="bg-deep-charcoal text-soft-gray font-sans overflow-x-hidden">
    <!-- Shared Navigation Header -->
    <header class="fixed top-0 left-0 right-0 z-50 glass-effect">
        <div class="max-w-7xl mx-auto px-6 py-4">
            <div class="flex items-center justify-between">
                <div class="flex items-center space-x-3">
                    <div class="w-8 h-8 bg-electric rounded-lg flex items-center justify-center">
                        <span class="text-navy font-bold text-sm">GPU</span>
                    </div>
                    <h1 class="text-xl font-bold gradient-text">GPU Calculator Pro</h1>
                </div>
                <nav class="hidden md:flex items-center space-x-8">
                    <a href="index.html#gpu" class="text-soft-gray hover:text-electric transition-colors">GPU</a>
                    <a href="index.html#models" class="text-soft-gray hover:text-electric transition-colors">Open Source
                        Models</a>
                    <a href="calculator.html"
                        class="text-soft-gray hover:text-electric transition-colors">Calculator</a>
                </nav>
            </div>
        </div>
    </header>



    <div class="overlay" id="overlay" onclick="closeShareDialog()"></div>
    <div class="explanation-dialog" id="explanationDialog">
        <h3>📊 How Max Concurrent Requests is Calculated</h3>

        <div class="explanation-content">
            <h4>The Formula:</h4>
            <div class="formula-box">
                <code>Max Concurrent Requests = Available Memory / KV Cache per Request</code>
            </div>

            <h4>Step-by-Step Breakdown:</h4>

            <div class="step">
                <strong>1. Total VRAM Available</strong>
                <code>Total VRAM = Number of GPUs × VRAM per GPU</code>
                <p>Example: 2 × 24GB = 48GB total</p>
            </div>

            <div class="step">
                <strong>2. Model Memory (Adjusted for Quantization)</strong>
                <code>Adjusted Model Memory = Base Memory × (bits after ÷ bits before)</code>
                <p>Per-parameter method: Parameters × Bytes per parameter</p>
                <p>Example (7B INT4): 14GB (FP16 baseline) × (4 ÷ 16) = <strong>3.5GB</strong></p>
                <p>Mapping: FP16/BF16 = 2 bytes; FP8/INT8 = 1 byte; INT4/MXFP4 = 0.5 bytes; INT2 = 0.25 bytes.</p>
                <p>The model weights are loaded once and stay in memory.</p>
            </div>

            <div class="step">
                <strong>3. KV Cache per Request</strong>
                <code>KV bytes = context_len × L × 2 × H × bytes_per_elem</code>
                <p>Where: <em>L</em> = transformer layers, <em>H</em> = hidden size, <em>2</em> = key + value, <em>bytes_per_elem</em> ≈ 2 (fp16/bf16).</p>
                <p>Example (7B, L=32, H=4096, fp16, 8K context): 8192 × 32 × 2 × 4096 × 2 bytes ≈ <strong>4.0GB</strong> raw KV. With 20% overhead → <strong>4.8GB</strong> per request.</p>
                <p>This memory is needed for each active request's attention cache.</p>
            </div>

            <div class="step">
                <strong>4. Available Memory for Inference</strong>
                <code>Available = Total VRAM - System Overhead - Model Memory</code>
                <p>Example: 48GB - 2GB - 3.5GB = <strong>42.5GB</strong></p>
                <p>This is what's left for KV caches after loading the model.</p>
            </div>

            <div class="step">
                <strong>5. Maximum Concurrent Requests</strong>
                <code>Max Requests = Available Memory / KV Cache per Request</code>
                <p>Example: 42.5GB / 4.8GB ≈ <strong>8.9</strong> requests</p>
            </div>

            <h4>What the Results Mean:</h4>
            <ul>
                <li><strong>&lt; 1 request:</strong> Can't handle full context length, need smaller context or better
                    GPU</li>
                <li><strong>1-2 requests:</strong> Basic serving capability, suitable for personal use</li>
                <li><strong>3-5 requests:</strong> Good for small-scale deployment</li>
                <li><strong>10+ requests:</strong> Production-ready for moderate traffic</li>
            </ul>

            <h4>Mixture-of-Experts (MoE) Models:</h4>
            <div class="step">
                <strong>Special Handling for MoE Models</strong>
                <p>MoE models (like Mixtral, DeepSeek V3/R1, Qwen3 MoE, Kimi K2, GLM-4.5) route tokens to a subset of experts per step.</p>
                <ul>
                    <li><strong>Total Parameters:</strong> Full model size (e.g., Mixtral 8x7B = 56B total).</li>
                    <li><strong>Active Parameters:</strong> Experts activated per token (e.g., ~14B active).</li>
                    <li><strong>Offloading Toggle:</strong> Use the "MoE Expert Offloading" checkbox to switch calculation modes:
                        <br>• Offloading <em>OFF</em> (default): VRAM and performance use <em>total</em> model size.
                        <br>• Offloading <em>ON</em>: VRAM and performance use <em>active experts</em>.</li>
                    <li><strong>Why:</strong> Popular frameworks (vLLM, SGLang) do not offload inactive experts today. We model both regimes to prepare for future offloading support.</li>
                </ul>
                <p><strong>Example:</strong> Mixtral 8x7B shows "~94GB total, ~16GB active". With offloading <em>ON</em>, both VRAM and tokens/sec use ~16GB (active experts). With offloading <em>OFF</em>, both VRAM and tokens/sec use ~94GB (full model).</p>
            </div>

            <h4>Important Notes:</h4>
            <ul>
                <li>This is a rough estimate - actual usage varies by model architecture</li>
                <li><strong>Assumes worst-case scenario:</strong> All requests use the full context window. In reality,
                    most requests use much less, so you may handle more concurrent requests</li>
                <li>KV cache grows linearly with actual tokens used, not maximum context</li>
                <li>Different attention mechanisms (MHA, MQA, GQA) affect memory usage</li>
                <li>Framework overhead and memory fragmentation can impact real-world performance</li>
                <li>Dynamic batching and memory management can improve real-world throughput</li>
                <li><strong>MoE models:</strong> Memory requirements can vary based on routing algorithms and expert
                    utilization patterns</li>
            </ul>
        </div>

        <div class="share-dialog-buttons">
            <button onclick="closeExplanationDialog()">✖ Close</button>
        </div>
    </div>

    <div class="performance-explanation-dialog" id="performanceExplanationDialog">
        <h3>⚡ How GPU Performance is Estimated</h3>

        <div class="explanation-content">
            <h4>The Formula:</h4>
            <div class="formula-box">
                <code>Tokens/sec = (Memory Bandwidth / Model Size) × Efficiency × Quantization Boost × Context Impact</code>
            </div>
            <p><strong>Verdict:</strong> ⚠️ Heuristic, not physically or computationally accurate.</p>
            <p><strong>Why:</strong></p>
            <ul>
                <li>Token generation speed is not directly proportional to <code>Memory Bandwidth / Model Size</code>.</li>
                <li>While memory bandwidth is a major bottleneck in large-model inference, compute throughput (FLOPs) and kernel scheduling also limit performance.</li>
                <li>Actual latency per token depends on matrix–vector multiplication performance, KV-cache access (bandwidth-bound), and framework efficiency (kernel fusion, parallelism).</li>
            </ul>
            <p>✅ <strong>Better framing:</strong> Use bandwidth as a dominant factor in memory-bound regimes (e.g., small batch, quantized, or large-context inference). Otherwise, both FLOPs and bandwidth matter. Treat this as a qualitative model.</p>

            <h4>Key Factors:</h4>

            <div class="step">
                <strong>1. Memory Bandwidth</strong>
                <p>GPU memory bandwidth is often the primary bottleneck for LLM inference when compute is underutilized (especially during decoding).</p>
                <ul>
                    <li>RTX 4090: 1,008 GB/s (typical)</li>
                    <li>A100 (80GB HBM2e): 2,039 GB/s (upper range)</li>
                    <li>H100 (80GB HBM3): ~3,350 GB/s</li>
                </ul>
                <p>✔ For large transformer inference, bandwidth is a critical limiting factor once compute is underutilized.</p>
            </div>

            <div class="step">
                <strong>2. Model Size Efficiency</strong>
                <p>Memory bandwidth utilization tends to drop for larger models because of increased layer depth, activation sizes, and less cache reuse; typical efficiency ranges 30–85% depending on model size and inference engine.</p>
                <p><strong>Notes:</strong></p>
                <ul>
                    <li>Smaller models can better fit in cache (L2, HBM, registers) → higher efficiency.</li>
                    <li>Utilization depends strongly on batch size, sequence length, and kernel fusion—not parameter count alone.</li>
                </ul>
            </div>

            <div class="step">
                <strong>3. Quantization Speed Boost</strong>
                <p>Lower precision primarily saves memory bandwidth and can accelerate GEMM ops when hardware supports low-precision tensor cores. Typical ranges on NVIDIA GPUs:</p>
                <ul>
                    <li>INT8: 1.3×–1.5× vs FP16</li>
                    <li>INT4: 1.6×–2.0× vs FP16 (kernel/framework dependent)</li>
                    <li>INT3: Experimental (few toolchains; speedups vary)</li>
                    <li>FP16: 1.0× baseline</li>
                </ul>
            </div>

            <div class="step">
                <strong>4. Context Length Impact</strong>
                <p>Throughput declines with longer context windows due to larger KV-cache bandwidth demands; typical degradation is 15–70% as context extends from 8K to 128K tokens.</p>
                <p><strong>Depends on:</strong> attention implementation (e.g., FlashAttention v1/v2/v3), batch size, and streaming optimizations.</p>
            </div>

            <div class="step">
                <strong>5. Multi-GPU Scaling</strong>
                <p>Multiple GPUs don't scale perfectly due to communication overhead:</p>
                <ul>
                    <li>2 GPUs: ~85% scaling efficiency</li>
                    <li>4 GPUs: ~75% scaling efficiency</li>
                    <li>8 GPUs: ~65% scaling efficiency</li>
                </ul>
            </div>

            <h4>Important Considerations:</h4>
            <ul>
                <li><strong>Framework matters (vLLM, TGI, Ollama):</strong> They differ in KV-cache management, batching, and fused kernels.</li>
                <li><strong>Batch size affects throughput:</strong> Larger batches → higher GPU utilization.</li>
                <li><strong>First token slower:</strong> Prompt prefill cost dominates.</li>
                <li><strong>Real-world variance:</strong> Throughput varies ±20% due to scheduling, framework, and CUDA version.</li>
                <li><strong>Professional GPUs:</strong> A100/H100 have better memory subsystems than consumer cards.</li>
            </ul>

            <h4>MoE Handling:</h4>
            <div class="step">
                <p>MoE models activate only a subset of experts per token. The calculator exposes an offloading toggle to align both memory and performance calculations:</p>
                <ul>
                    <li><strong>Offloading OFF:</strong> Tokens/sec and VRAM use <em>total</em> model size.</li>
                    <li><strong>Offloading ON:</strong> Tokens/sec and VRAM use <em>active experts</em>.</li>
                </ul>
                <p><em>Note:</em> Real-world behavior depends on your framework. This mode assumes inactive experts are offloaded and not consuming bandwidth or VRAM.</p>
            </div>

            <h4>Performance Ratings:</h4>
            <ul>
                <li>🟢 <strong>Excellent (>100 tok/s):</strong> Real-time conversation, instant responses</li>
                <li>🟢 <strong>Good (50-100 tok/s):</strong> Smooth interaction, minimal wait</li>
                <li>🟡 <strong>Moderate (25-50 tok/s):</strong> Acceptable for most uses, some waiting</li>
                <li>🟡 <strong>Slow (10-25 tok/s):</strong> Noticeable delays, patience required</li>
                <li>🔴 <strong>Very Slow (<10 tok/s):</strong> Long waits, consider optimization</li>
            </ul>
        </div>

        <div class="share-dialog-buttons">
            <button onclick="closePerformanceExplanation()">✖ Close</button>
        </div>
    </div>

    <div class="container mt-24">



        <div class="calculator-grid">
            <div class="section card">
                <h2>› Hardware Configuration</h2>
                <div class="form-group accent-box">
                    <label for="gpu-type" class="field-title">GPU Model:</label>
                    <select id="gpu-type" onchange="updateGPUSpecs();">
                        <option value="">Select GPU...</option>
                        <optgroup label="NVIDIA Professional">
                            <option value="a100" data-vram="40">A100 40GB (40GB VRAM)</option>
                            <option value="a100-80" data-vram="80">A100 80GB (80GB VRAM)</option>
                            <option value="h100" data-vram="80">H100 (80GB VRAM)</option>
                        </optgroup>
                    </select>
                </div>

                <div class="form-group accent-box">
                    <label for="gpu-count" class="field-title">Number of GPUs:</label>
                    <input type="number" id="gpu-count" min="1" max="8" value="1" oninput="calculate();">
                </div>

                <div class="form-group accent-box">
                    <label for="vram-per-gpu" class="field-title">VRAM per GPU (GB):</label>
                    <input type="number" id="vram-per-gpu" min="4" max="128" value="24" readonly>
                </div>

                <div class="form-group accent-box">
                    <label for="system-overhead" class="field-title">System Overhead (GB):</label>
                    <input type="number" id="system-overhead" min="0" max="8" value="2" step="0.5"
                        oninput="calculate();">
                </div>
            </div>

            <div class="section card">
                <h2>› Model Configuration</h2>

                <div class="form-group radio-group accent-box">
                    <label class="radio-group-label">Model Input Method:</label>
                    <div class="radio-options">
                        <label class="radio-option">
                            <input type="radio" name="model-input-type" value="preset" checked
                                onchange="updateModelInputMethod()">
                            <span>Common Model</span>
                        </label>
                        <label class="radio-option">
                            <input type="radio" name="model-input-type" value="parameters"
                                onchange="updateModelInputMethod()">
                            <span>Parameters</span>
                        </label>
                        <label class="radio-option">
                            <input type="radio" name="model-input-type" value="memory"
                                onchange="updateModelInputMethod()">
                            <span>Memory</span>
                        </label>
                    </div>
                </div>

                <div class="form-group accent-box" id="model-preset-group">
                    <label for="model-preset" class="field-title">Select Model:</label>
                    <select id="model-preset" onchange="updateModelSelection()">
                        <optgroup label="Meta Llama">
                            <option value="7" data-memory="14" data-quant="1.0">Llama 3 8B (~14GB)</option>
                            <option value="70" data-memory="140" data-quant="1.0">Llama 3 70B (~140GB)</option>
                            <option value="70" data-memory="140" data-quant="1.0">Llama 3.1 70B (~140GB)</option>
                            <option value="405" data-memory="810" data-quant="0.5">Llama 3.1 405B (~810GB)</option>
                        </optgroup>
                        <optgroup label="Alibaba Qwen 3 (2025)">
                            <option value="4" data-memory="8" data-quant="1.0">Qwen3 4B (~8GB)</option>
                            <option value="8" data-memory="16" data-quant="1.0">Qwen3 8B (~16GB)</option>
                            <option value="14" data-memory="28" data-quant="1.0">Qwen3 14B (~28GB)</option>
                            <option value="32" data-memory="64" data-quant="1.0">Qwen3 32B (~64GB)</option>
                            <option value="3" data-memory="60" data-active-memory="6" data-quant="1.0">Qwen3 30B-A3B MoE
                                (~60GB total, ~6GB active)</option>
                            <option value="22" data-memory="470" data-active-memory="44" data-quant="0.5">Qwen3
                                235B-A22B MoE (~470GB total, ~44GB active)</option>
                        </optgroup>
                        <optgroup label="Moonshot AI Kimi (2025)">
                            <option value="32" data-memory="2000" data-active-memory="64" data-quant="0.5">Kimi K2 Base
                                1T (~2000GB total, ~64GB active)</option>
                            <option value="32" data-memory="2000" data-active-memory="64" data-quant="0.5">Kimi K2
                                Instruct 1T (~2000GB total, ~64GB active)</option>
                        </optgroup>
                        <optgroup label="Qwen Coder">
                            <option value="1.5" data-memory="3" data-quant="1.0">Qwen-Coder 1.5B (~3GB)</option>
                            <option value="7" data-memory="14" data-quant="1.0">Qwen-Coder 7B (~14GB)</option>
                            <option value="32" data-memory="64" data-quant="0.5">Qwen-Coder 32B (~64GB)</option>
                        </optgroup>
                        <optgroup label="DeepSeek">
                            <option value="7" data-memory="14" data-quant="1.0">DeepSeek 7B (~14GB)</option>
                            <option value="16" data-memory="32" data-quant="1.0">DeepSeek 16B (~32GB)</option>
                            <option value="67" data-memory="134" data-quant="0.5">DeepSeek 67B (~134GB)</option>
                            <option value="37" data-memory="1342" data-active-memory="74" data-quant="0.5">DeepSeek V3
                                671B (~1342GB total, ~74GB active)</option>
                            <option value="37" data-memory="1342" data-active-memory="74" data-quant="0.5">DeepSeek V3.1
                                671B (~1342GB total, ~74GB active)</option>
                            <option value="37" data-memory="1342" data-active-memory="74" data-quant="0.5">DeepSeek R1
                                671B (~1342GB total, ~74GB active)</option>
                            <option value="1.5" data-memory="3" data-quant="1.0">DeepSeek R1 1.5B Distilled (~3GB)
                            </option>
                            <option value="7" data-memory="14" data-quant="1.0">DeepSeek R1 7B Distilled (~14GB)
                            </option>
                            <option value="32" data-memory="64" data-quant="1.0">DeepSeek R1 32B Distilled (~64GB)
                            </option>
                            <option value="70" data-memory="140" data-quant="0.5">DeepSeek R1 70B Distilled (~140GB)
                            </option>
                        </optgroup>
                        <optgroup label="Mistral">
                            <option value="7" data-memory="14" data-quant="1.0">Mistral 7B (~14GB)</option>
                            <option value="12" data-memory="24" data-quant="1.0">Mistral-Nemo 12B (~24GB)</option>
                            <option value="22" data-memory="44" data-quant="0.5">Mistral-Small 22B (~44GB)</option>
                            <option value="123" data-memory="246" data-quant="0.5">Mistral-Large 123B (~246GB)</option>
                        </optgroup>
                        <optgroup label="Mistral Voxtral">
                            <option value="3" data-memory="6" data-quant="1.0">Mistral Voxtral 3B (~6GB)</option>
                            <option value="24" data-memory="48" data-quant="1.0">Mistral Voxtral 24B (~48GB)</option>
                        </optgroup>
                        <optgroup label="Mixtral">
                            <option value="8" data-memory="94" data-active-memory="16" data-quant="0.5">Mixtral 8x7B
                                (~94GB total, ~16GB active)</option>
                            <option value="8" data-memory="282" data-active-memory="45" data-quant="0.5">Mixtral 8x22B
                                (~282GB total, ~45GB active)</option>
                        </optgroup>
                        <optgroup label="Mistral Codestral">
                            <option value="22" data-memory="44" data-quant="0.5">Codestral 22B (~44GB)</option>
                            <option value="100" data-memory="200" data-quant="0.5">Codestral 25.01 (~200GB)</option>
                            <option value="100" data-memory="200" data-quant="0.5">Codestral 25.08 (~200GB)</option>
                        </optgroup>
                        <optgroup label="OpenAI GPT">
                            <option value="20.9" data-memory="42" data-quant="0.3">GPT-OSS 20B (~42GB FP16, ~16GB MXFP4)
                            </option>
                            <option value="116.8" data-memory="234" data-quant="0.3">GPT-OSS 120B (~234GB FP16, ~61GB
                                MXFP4)</option>
                            <option value="6" data-memory="12" data-quant="1.0">GPT-J 6B (~12GB)</option>
                            <option value="2.7" data-memory="5.4" data-quant="1.0">GPT-Neo 2.7B (~5.4GB)</option>
                            <option value="20" data-memory="40" data-quant="0.5">GPT-NeoX 20B (~40GB)</option>
                        </optgroup>
                        <optgroup label="Embeddings & Rerankers">
                            <option value="0.56" data-memory="1.12" data-quant="1.0">BGE-M3 560M (~1.12GB)</option>
                            <option value="8" data-memory="16" data-quant="1.0">Qwen3 Embedding 8B (~16GB)</option>
                            <option value="4" data-memory="8" data-quant="1.0">Qwen3 Embedding 4B (~8GB)</option>
                            <option value="0.6" data-memory="1.2" data-quant="1.0">Qwen3 Embedding 0.6B (~1.2GB)</option>
                            <option value="8" data-memory="16" data-quant="1.0">Qwen3 Reranker 8B (~16GB)</option>
                            <option value="4" data-memory="8" data-quant="1.0">Qwen3 Reranker 4B (~8GB)</option>
                            <option value="0.6" data-memory="1.2" data-quant="1.0">Qwen3 Reranker 0.6B (~1.2GB)</option>
                        </optgroup>
                        <optgroup label="Zhipu AI GLM">
                            <option value="6.2" data-memory="12" data-quant="1.0">ChatGLM-6B (~12GB)</option>
                            <option value="6.2" data-memory="12" data-quant="1.0">ChatGLM2-6B (~12GB)</option>
                            <option value="9" data-memory="18" data-quant="1.0">GLM-4-9B (~18GB)</option>
                            <option value="32" data-memory="64" data-quant="1.0">GLM-4-32B (~64GB)</option>
                            <option value="12" data-memory="106" data-active-memory="24" data-quant="0.5">GLM-4.5-Air
                                MoE (~106GB total, ~24GB active)</option>
                            <option value="32" data-memory="355" data-active-memory="64" data-quant="0.5">GLM-4.5 MoE
                                (~355GB total, ~64GB active)</option>
                        </optgroup>
                        <optgroup label="Google Gemma 3 (2025)">
                            <option value="0.27" data-memory="0.5" data-quant="1.0">Gemma 3 270M (~0.5GB)</option>
                            <option value="1" data-memory="2" data-quant="1.0">Gemma 3 1B (~2GB)</option>
                            <option value="4" data-memory="8" data-quant="1.0">Gemma 3 4B (~8GB)</option>
                            <option value="12" data-memory="24" data-quant="1.0">Gemma 3 12B (~24GB)</option>
                            <option value="27" data-memory="54" data-quant="0.5">Gemma 3 27B (~54GB)</option>
                        </optgroup>
                        <optgroup label="Microsoft">
                            <option value="3.8" data-memory="7.6" data-quant="1.0">Phi-3 Mini 3.8B (~7.6GB)</option>
                            <option value="7" data-memory="14" data-quant="1.0">Phi-3 Small 7B (~14GB)</option>
                            <option value="14" data-memory="28" data-quant="1.0">Phi-3 Medium 14B (~28GB)</option>
                        </optgroup>
                        <optgroup label="Other Models">
                            <option value="6" data-memory="12" data-quant="1.0">Yi 6B (~12GB)</option>
                            <option value="34" data-memory="68" data-quant="0.5">Yi 34B (~68GB)</option>
                            <option value="34" data-memory="68" data-quant="0.5">CodeLlama 34B (~68GB)</option>
                            <option value="40" data-memory="80" data-quant="0.5">Falcon 40B (~80GB)</option>
                            <option value="180" data-memory="360" data-quant="0.5">Falcon 180B (~360GB)</option>
                        </optgroup>
                    </select>
                </div>

                <div class="form-group hidden" id="model-parameters-group">
                    <label for="model-parameters">Model Parameters (B):</label>
                    <input type="number" id="model-parameters" min="0.1" max="1000" step="0.1" value="7"
                        oninput="calculate();">
                    <small class="help-text">Enter billions of parameters (e.g., 7 for 7B model)</small>
                </div>

                <div class="form-group hidden" id="model-memory-group">
                    <label for="model-memory-input">Model Memory (GB):</label>
                    <input type="number" id="model-memory-input" min="1" max="2000" value="14" oninput="calculate();">
                    <small class="help-text">Enter total memory required in GB</small>
                </div>

                <div class="form-group accent-box">
                    <label for="quantization" class="field-title">Quantization:</label>
                    <select id="quantization" onchange="calculate();">
                        <option value="1.0">FP16/BF16 (2 bytes/param, 0% reduction)</option>
                        <option value="0.5">INT8 / FP8 (1 byte/param, 50% reduction)</option>
                        <option value="0.25">INT4 / MXFP4 (0.5 bytes/param, 75% reduction)</option>
                        <option value="0.125">INT2 (0.25 bytes/param, 87.5% reduction)</option>
                    </select>
                </div>

                <div class="form-group accent-box">
                    <label for="moe-offloading" class="field-title">MoE Expert Offloading:</label>
                    <div class="radio-options" style="align-items:center;">
                        <label class="radio-option" style="display:flex;gap:8px;align-items:center;">
                            <input type="checkbox" id="moe-offloading" oninput="calculate();">
                            <span>Enable offloading for MoE models</span>
                        </label>
                    </div>
                    <small class="help-text">Default: off. When disabled, MoE models use <em>total memory</em> for both VRAM and performance. When enabled, MoE models use <em>active experts</em> for both VRAM and performance. Applies only to MoE presets (e.g., Mixtral, DeepSeek V3/R1, Kimi K2, GLM-4.5).</small>
                </div>

                <div class="form-group radio-group accent-box">
                    <label class="radio-group-label">Context Input Method:</label>
                    <div class="radio-options">
                        <label class="radio-option">
                            <input type="radio" name="context-input-type" value="preset" checked
                                onchange="updateContextInputMethod()">
                            <span>Preset</span>
                        </label>
                        <label class="radio-option">
                            <input type="radio" name="context-input-type" value="custom"
                                onchange="updateContextInputMethod()">
                            <span>Custom</span>
                        </label>
                    </div>
                </div>

                <div class="form-group accent-box" id="context-preset-group">
                    <label for="context-preset" class="field-title">Context Length:</label>
                    <select id="context-preset" onchange="calculate();">
                        <option value="1024">1K tokens</option>
                        <option value="2048">2K tokens</option>
                        <option value="4096">4K tokens</option>
                        <option value="8192">8K tokens</option>
                        <option value="16384">16K tokens</option>
                        <option value="32768">32K tokens</option>
                        <option value="65536">64K tokens</option>
                        <option value="131072">128K tokens</option>
                        <option value="262144">256K tokens</option>
                        <option value="524288">512K tokens</option>
                        <option value="1048576">1M tokens</option>
                    </select>
                </div>

                <div class="form-group hidden" id="context-custom-group">
                    <label for="context-custom">Context Length (tokens):</label>
                    <input type="number" id="context-custom" min="128" max="2000000" value="4096"
                        oninput="calculate();">
                    <small class="help-text">Enter number of tokens</small>
                </div>

                <div class="form-group accent-box">
                    <label for="kv-cache-overhead" class="field-title">KV Cache Overhead (%):</label>
                    <input type="number" id="kv-cache-overhead" min="10" max="50" value="20" step="5"
                        oninput="calculate();">
                    <small class="help-text">Use arrow keys to adjust</small>
                </div>
            </div>
        </div>



        <div id="results" class="results hidden">
            <div class="results-title">
                <span class="panel-title">Calculation Results</span>
                <button type="button" class="btn-link" onclick="showHowCalculated(event)">How is this
                    calculated?</button>
            </div>

            <div class="result-item highlight-result">
                <span class="result-label">Max Concurrent Requests:</span>
                <span class="result-value" id="concurrent-requests">-</span>
            </div>

            <div class="result-item">
                <span class="result-label">Total VRAM Available:</span>
                <span class="result-value" id="total-vram">-</span>
            </div>

            <div class="result-item">
                <span class="result-label">Model Memory Required:</span>
                <span class="result-value" id="model-memory">-</span>
            </div>

            <div class="result-item">
                <span class="result-label">KV Cache per Request:</span>
                <span class="result-value" id="kv-cache-memory">-</span>
            </div>

            <div class="result-item">
                <span class="result-label">Available for Inference:</span>
                <span class="result-value" id="available-memory">-</span>
            </div>

            <div class="result-item">
                <span class="result-label">Effective Context Window:</span>
                <span class="result-value" id="effective-context">-</span>
            </div>

            <div id="warnings"></div>



            <!-- Performance Estimation Section -->
            <div id="performance-section" class="performance-section accent-box" style="display: none;">
                <div class="performance-title">
                    <span class="panel-title">› Performance Estimation</span>
                    <a href="#" class="btn-link" onclick="showPerformanceExplanation(event)">How is this calculated?</a>
                </div>

                <div class="performance-result">
                    <div class="performance-metric">
                        <span class="metric-label">Expected Speed:</span>
                        <span class="metric-value" id="tokens-per-second">-</span>
                    </div>

                    <div class="performance-metric">
                        <span class="metric-label">Time for 100 tokens:</span>
                        <span class="metric-value" id="generation-time">-</span>
                    </div>

                    <div class="performance-metric">
                        <span class="metric-label">Performance Rating:</span>
                        <span class="metric-value" id="performance-rating">-</span>
                    </div>
                </div>

                <div id="performance-notes" class="performance-notes"></div>
            </div>
        </div>

        <!-- Performance Scenarios Table -->
        <div id="scenario-table-section" class="performance-section accent-box" style="display: none;">
            <div class="performance-title" style="display:flex; align-items:center;">
                <span class="panel-title" style="font-weight:700;">› Performance Scenarios</span>
                <div style="flex:1"></div>
                <div style="display:flex; gap:8px;">
                    <button class="btn-link" onclick="copyScenarioTable()">Copy</button>
                    <button class="btn-link" onclick="downloadScenarioTable()">Download CSV</button>
                </div>
            </div>
            <!-- Filters -->
            <div id="scenario-filters" class="mt-2 mb-2" style="display:grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap:12px; align-items:center;">
                <div style="display:flex; gap:8px; align-items:center;">
                    <label class="text-soft-gray/80" for="scenario-filter-context">Context:</label>
                    <select id="scenario-filter-context" class="bg-deep-charcoal text-soft-gray px-2 py-1 rounded w-full">
                        <option value="all">All</option>
                        <option value="8192">8K</option>
                        <option value="16384">16K</option>
                        <option value="32768">32K</option>
                        <option value="65536">64K</option>
                        <option value="131072">128K</option>
                    </select>
                </div>
                <div style="display:flex; gap:8px; align-items:center;">
                    <label class="text-soft-gray/80" for="scenario-filter-min-tps">Min tokens/sec:</label>
                    <input id="scenario-filter-min-tps" type="number" min="0" step="0.1" placeholder="e.g. 10" class="bg-deep-charcoal text-soft-gray px-2 py-1 rounded w-full" />
                </div>
            </div>
            <style>
                #scenario-table { border-collapse: collapse; }
                #scenario-table th, #scenario-table td { border-bottom: 1px solid rgba(226, 232, 240, 0.2); }
                #scenario-table thead th { font-weight: 700; }
                #scenario-table thead th .sort-icons { display: inline-flex; flex-direction: column; margin-left: 6px; font-size: 10px; line-height: 10px; }
                #scenario-table thead th .sort-icons span { opacity: 0.35; }
                #scenario-table thead th.sorted-asc .sort-icons .up { opacity: 1; }
                #scenario-table thead th.sorted-desc .sort-icons .down { opacity: 1; }
            </style>
            <div class="overflow-x-auto">
                <table id="scenario-table" class="w-full text-left text-soft-gray">
                    <thead>
                        <tr>
                            <th class="py-2 pr-3 cursor-pointer" data-key="model" title="Click to sort"><span>Model</span><span class="sort-icons"><span class="up">▲</span><span class="down">▼</span></span></th>
                            <th class="py-2 pr-3 cursor-pointer" data-key="gpu" title="Click to sort"><span>GPU</span><span class="sort-icons"><span class="up">▲</span><span class="down">▼</span></span></th>
                            <th class="py-2 pr-3 cursor-pointer" data-key="gpuCount" title="Click to sort"><span># GPUs</span><span class="sort-icons"><span class="up">▲</span><span class="down">▼</span></span></th>
                            <th class="py-2 pr-3 cursor-pointer" data-key="quant" title="Click to sort"><span>Quantization</span><span class="sort-icons"><span class="up">▲</span><span class="down">▼</span></span></th>
                            <th class="py-2 pr-3 cursor-pointer" data-key="context" title="Click to sort"><span>Context</span><span class="sort-icons"><span class="up">▲</span><span class="down">▼</span></span></th>
                            <th class="py-2 pr-3 cursor-pointer" data-key="maxConcurrent" title="Click to sort"><span>Max Concurrent</span><span class="sort-icons"><span class="up">▲</span><span class="down">▼</span></span></th>
                            <th class="py-2 pr-3 cursor-pointer" data-key="tokensPerSec" title="Click to sort"><span>Tokens/sec</span><span class="sort-icons"><span class="up">▲</span><span class="down">▼</span></span></th>
                            <th class="py-2 pr-3 cursor-pointer" data-key="genTimeNum" title="Click to sort"><span>Time for 100 tokens</span><span class="sort-icons"><span class="up">▲</span><span class="down">▼</span></span></th>
                        </tr>
                    </thead>
                    <tbody>
                        <!-- Populated dynamically -->
                    </tbody>
                </table>
            </div>
            <div class="help-text mt-2">Auto-generated from current selections (MoE mode applies).</div>
        </div>

        <div class="info-box">
            <div class="info-title">› CALCULATION NOTES</div>
            • Model memory includes weights and activation overhead<br>
            • KV cache grows linearly with context length and batch size<br>
            • System overhead includes OS, drivers, and framework memory<br>
            • Quantization reduces precision but may impact model quality<br>
            • Professional GPUs (A100, H100) have better memory bandwidth<br>
            • Multiple GPUs require tensor parallelism for large models
        </div>

        <footer class="footer">
            <div class="footer-content">
                <div class="copyright">
                    Copyright © 2025 Eran Sandler
                </div>
                <div class="social-links">
                    <a href="https://eran.sandler.co.il" target="_blank" rel="noopener" title="Website"
                        class="social-link">
                        <svg width="20" height="20" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                            <path
                                d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm-1 17.93c-3.94-.49-7-3.85-7-7.93 0-.62.08-1.21.21-1.79L9 15v1c0 1.1.9 2 2 2v1.93zm6.9-2.54c-.26-.81-1-1.39-1.9-1.39h-1v-3c0-.55-.45-1-1-1H8v-2h2c.55 0 1-.45 1-1V7h2c1.1 0 2-.9 2-2v-.41c2.93 1.19 5 4.06 5 7.41 0 2.08-.8 3.97-2.1 5.39z"
                                fill="currentColor" />
                        </svg>
                    </a>
                    <a href="https://x.com/erans" target="_blank" rel="noopener" title="X (Twitter)"
                        class="social-link">
                        <svg width="20" height="20" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                            <path
                                d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"
                                fill="currentColor" />
                        </svg>
                    </a>
                    <a href="https://bsky.app/profile/esandler.bsky.social" target="_blank" rel="noopener"
                        title="BlueSky" class="social-link">
                        <svg width="20" height="20" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                            <path
                                d="M12 10.8c-1.087-2.114-4.046-6.053-6.798-7.995C2.566.944 1.561 1.266.902 1.565.139 1.908 0 3.08 0 3.768c0 .69.378 5.65.624 6.479.815 2.736 3.713 3.66 6.383 3.364.136-.02.275-.039.415-.056-.138.022-.276.04-.415.056-3.912.58-7.387 2.005-2.83 7.078 5.013 5.19 6.87-1.113 7.823-4.308.953 3.195 2.05 9.271 7.733 4.308 4.267-4.308 1.172-6.498-2.74-7.078a8.741 8.741 0 0 1-.415-.056c.14.017.279.036.415.056 2.67.297 5.568-.628 6.383-3.364.246-.828.624-5.79.624-6.478 0-.69-.139-1.861-.902-2.206-.659-.298-1.664-.62-4.3 1.24C16.046 4.748 13.087 8.687 12 10.8Z"
                                fill="currentColor" />
                        </svg>
                    </a>
                    <a href="https://github.com/erans/selfhostllm" target="_blank" rel="noopener" title="GitHub"
                        class="social-link">
                        <svg width="20" height="20" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                            <path
                                d="M12 2C6.477 2 2 6.477 2 12c0 4.42 2.865 8.17 6.839 9.49.5.092.682-.217.682-.482 0-.237-.008-.866-.013-1.7-2.782.603-3.369-1.342-3.369-1.342-.454-1.155-1.11-1.462-1.11-1.462-.908-.62.069-.608.069-.608 1.003.07 1.531 1.03 1.531 1.03.892 1.529 2.341 1.087 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.11-4.555-4.943 0-1.091.39-1.984 1.029-2.683-.103-.253-.446-1.27.098-2.647 0 0 .84-.269 2.75 1.025A9.578 9.578 0 0112 6.836c.85.004 1.705.114 2.504.337 1.909-1.294 2.747-1.025 2.747-1.025.546 1.377.203 2.394.1 2.647.64.699 1.028 1.592 1.028 2.683 0 3.842-2.339 4.687-4.566 4.935.359.309.678.919.678 1.852 0 1.336-.012 2.415-.012 2.743 0 .267.18.578.688.48C19.138 20.167 22 16.418 22 12c0-5.523-4.477-10-10-10z"
                                fill="currentColor" />
                        </svg>
                    </a>
                    <a href="https://linkedin.com/in/erans" target="_blank" rel="noopener" title="LinkedIn"
                        class="social-link">
                        <svg width="20" height="20" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                            <path
                                d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"
                                fill="currentColor" />
                        </svg>
                    </a>
                    <a href="https://github.com/erans/selfhostllm" target="_blank" rel="noopener"
                        class="github-fork-link">
                        <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"
                            style="margin-right: 5px;">
                            <path
                                d="M12 2C6.477 2 2 6.477 2 12c0 4.42 2.865 8.17 6.839 9.49.5.092.682-.217.682-.482 0-.237-.008-.866-.013-1.7-2.782.603-3.369-1.342-3.369-1.342-.454-1.155-1.11-1.462-1.11-1.462-.908-.62.069-.608.069-.608 1.003.07 1.531 1.03 1.531 1.03.892 1.529 2.341 1.087 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.11-4.555-4.943 0-1.091.39-1.984 1.029-2.683-.103-.253-.446-1.27.098-2.647 0 0 .84-.269 2.75 1.025A9.578 9.578 0 0112 6.836c.85.004 1.705.114 2.504.337 1.909-1.294 2.747-1.025 2.747-1.025.546 1.377.203 2.394.1 2.647.64.699 1.028 1.592 1.028 2.683 0 3.842-2.339 4.687-4.566 4.935.359.309.678.919.678 1.852 0 1.336-.012 2.415-.012 2.743 0 .267.18.578.688.48C19.138 20.167 22 16.418 22 12c0-5.523-4.477-10-10-10z"
                                fill="currentColor" />
                        </svg>
                        Fork on GitHub
                    </a>
                </div>
            </div>
        </footer>

        <!-- Shared Footer (from GPUs & LLMs page) -->
        <footer class="py-12 bg-deep-charcoal">
            <div class="max-w-7xl mx-auto px-6">
                <div class="text-center">
                    <div class="flex items-center justify-center space-x-3 mb-4">
                        <div class="w-8 h-8 bg-electric rounded-lg flex items-center justify-center">
                            <span class="text-navy font-bold text-sm">GPU</span>
                        </div>
                        <h4 class="text-xl font-bold gradient-text">GPU Calculator Pro</h4>
                    </div>
                    <p class="text-soft-gray/70 mb-6">
                        Advanced GPU requirements calculator for AI and ML workloads.
                        <br>
                        Built with precision calculations and professional-grade accuracy.
                    </p>
                    <div class="border-t border-soft-gray/20 pt-6">
                        <p class="text-sm text-soft-gray/50">
                            © 2024 GPU Calculator Pro. Built for developers, by developers.
                        </p>
                    </div>
                </div>
            </div>
        </footer>
    </div>

    <script src="selfhost-llm.js"></script>
</body>

</html>