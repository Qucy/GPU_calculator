[
  {
    "model_name": "Llama 3 70B",
    "release_date": "2024-04",
    "organization": "Meta AI",
    "architecture_type": "Dense",
    "parameter_count_billion": 70,
    "num_layers": null,
    "hidden_size": null,
    "num_attention_heads": null,
    "vocab_size": null,
    "precision_supported": ["BF16", "FP8"],
    "context_length": 8192,
    "quantization_available": true,
    "quantization_types": ["FP8", "INT8", "4-bit"],
    "moe": {
      "enabled": false,
      "num_experts": null,
      "active_experts": null,
      "expert_parallelism": null
    },
    "serving_frameworks": ["TensorRT-LLM", "vLLM", "llama.cpp"],
    "recommended_gpu": ["NVIDIA A100", "NVIDIA H100", "NVIDIA H200"],
    "throughput_tokens_per_sec_per_gpu": 700,
    "batch_size_tested": null,
    "sequence_length_tested": 8192,
    "memory_footprint_gb": null,
    "license": "Llama 3 Community License Agreement",
    "source_links": [
      "huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct",
      "nvidia.com"
    ],
    "notes": "High-throughput, general-purpose benchmark"
  },
  {
    "model_name": "Qwen3-235B-A22B",
    "release_date": "2025-04",
    "organization": "Alibaba Cloud / Qwen team",
    "architecture_type": "MoE",
    "parameter_count_billion": 235,
    "num_layers": null,
    "hidden_size": null,
    "num_attention_heads": null,
    "vocab_size": null,
    "precision_supported": ["BF16", "FP8"],
    "context_length": 256000,
    "quantization_available": true,
    "quantization_types": ["1-bit", "2-bit", "3-bit", "4-bit", "5-bit", "6-bit", "7-bit", "8-bit"],
    "moe": {
      "enabled": true,
      "num_experts": 128,
      "active_experts": 8,
      "expert_parallelism": true
    },
    "serving_frameworks": ["vLLM", "TensorRT-LLM", "SGLang", "Ollama", "llama.cpp"],
    "recommended_gpu": ["A100", "H100", "H200"],
    "throughput_tokens_per_sec_per_gpu": null,
    "batch_size_tested": null,
    "sequence_length_tested": 32768,
    "memory_footprint_gb": null,
    "license": "Apache 2.0",
    "source_links": ["qwen.ai", "huggingface.co"],
    "notes": "Dual-mode reasoning, advanced multilingualism"
  },
  {
    "model_name": "GLM-4.5 355B-A32B",
    "release_date": "2025-08",
    "organization": "Zhipu AI / THUDM",
    "architecture_type": "MoE",
    "parameter_count_billion": 355,
    "num_layers": null,
    "hidden_size": "CONFLICT",
    "num_attention_heads": "CONFLICT",
    "vocab_size": null,
    "precision_supported": ["BF16", "FP8"],
    "context_length": 128000,
    "quantization_available": false,
    "quantization_types": [],
    "moe": {
      "enabled": true,
      "num_experts": "CONFLICT",
      "active_experts": 32,
      "expert_parallelism": null
    },
    "serving_frameworks": ["Transformers", "vLLM", "SGLang"],
    "recommended_gpu": ["8x H100"],
    "throughput_tokens_per_sec_per_gpu": null,
    "batch_size_tested": null,
    "sequence_length_tested": 128000,
    "memory_footprint_gb": null,
    "license": "MIT open-source license",
    "source_links": ["huggingface.co", "arxiv.org"],
    "notes": "Dual-mode, excels in coding and agentic tasks"
  },
  {
    "model_name": "DeepSeek R1 671B-A37B",
    "release_date": "2025-05",
    "organization": "DeepSeek",
    "architecture_type": "MoE",
    "parameter_count_billion": 671,
    "num_layers": 64,
    "hidden_size": 4096,
    "num_attention_heads": 64,
    "vocab_size": 300000,
    "precision_supported": ["FP16", "BF16", "FP8"],
    "context_length": 128000,
    "quantization_available": true,
    "quantization_types": ["8-bit"],
    "moe": {
      "enabled": true,
      "num_experts": 256,
      "active_experts": 8,
      "expert_parallelism": "tensor parallel"
    },
    "serving_frameworks": ["SGLang", "vLLM", "Transformers"],
    "recommended_gpu": ["H100", "H200", "MI300X"],
    "throughput_tokens_per_sec_per_gpu": null,
    "batch_size_tested": null,
    "sequence_length_tested": 128000,
    "memory_footprint_gb": 260,
    "license": "MIT",
    "source_links": ["huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp"],
    "notes": "Superior mathematical and logical reasoning"
  },
  {
    "model_name": "Kimi K2 1T-A32B",
    "release_date": "2025-07",
    "organization": "Moonshot AI",
    "architecture_type": "MoE",
    "parameter_count_billion": 1000,
    "num_layers": 61,
    "hidden_size": 7168,
    "num_attention_heads": 64,
    "vocab_size": 160000,
    "precision_supported": ["FP16", "BF16", "FP8"],
    "context_length": 65536,
    "quantization_available": true,
    "quantization_types": ["4-bit", "8-bit"],
    "moe": {
      "enabled": true,
      "num_experts": 384,
      "active_experts": 8,
      "expert_parallelism": null
    },
    "serving_frameworks": ["vLLM", "SGLang", "KTransformers", "TensorRT-LLM"],
    "recommended_gpu": ["A100", "H100", "Mi300X"],
    "throughput_tokens_per_sec_per_gpu": null,
    "batch_size_tested": null,
    "sequence_length_tested": 65536,
    "memory_footprint_gb": null,
    "license": "Modified MIT",
    "source_links": [
      "https://huggingface.co/moonshotai/Kimi-K2-Instruct",
      "https://www.scmp.com/tech/tech-news/article/3268644/chinese-ai-firm-moonshot-ai-beefed-up-sparsely-gated-llm"
    ],
    "notes": "Trillion-parameter scale, agentic intelligence"
  }
]
